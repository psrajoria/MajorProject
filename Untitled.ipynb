{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14186fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5513c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {\"a\": 1, \"b\": 2, \"c\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479513f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from category_encoders import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.grid_search import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "###\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier, Booster\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2021\n",
    "PROBAS = True\n",
    "FOLDS = 5\n",
    "N_ESTIMATORS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ba6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_data = pd.read_csv(\"drug200.csv\")\n",
    "drug_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = drug_data.drop([\"Drug\"], axis=1)\n",
    "y = drug_data[\"Drug\"]\n",
    "\n",
    "print(f\"X:{X.shape} y: {y.shape} \\n\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_SEED\n",
    ")\n",
    "print(f\"X_train:{X_train.shape} y_train: {y_train.shape}\")\n",
    "print(f\"X_test:{X_test.shape} y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = drug_data[len(drug_data):].drop([\"Drug\"], axis = 1)\n",
    "# print (f'test:{test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12383620",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894488db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"n_estimators\": 10000,\n",
    "    \"objective\": \"binary\",\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"min_child_samples\": 150,\n",
    "    \"reg_alpha\": 3e-5,\n",
    "    \"reg_lambda\": 9e-2,\n",
    "    \"num_leaves\": 20,\n",
    "    \"max_depth\": 16,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"subsample\": 0.8,\n",
    "    \"subsample_freq\": 2,\n",
    "    \"max_bin\": 240,\n",
    "    \"device\": \"gpu\",\n",
    "}\n",
    "\n",
    "cb_params = {\n",
    "    \"max_depth\": 6,\n",
    "    \"max_ctr_complexity\": 5,\n",
    "    \"num_trees\": 50000,\n",
    "    \"od_wait\": 500,\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"learning_rate\": 0.04,\n",
    "    \"min_data_in_leaf\": 3,\n",
    "    \"task_type\": \"GPU\",\n",
    "}\n",
    "\n",
    "\n",
    "rf_params = {\"max_depth\": 15, \"min_samples_leaf\": 8, \"random_state\": RANDOM_SEED}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl1 = KNeighborsClassifier(n_neighbors=1)\n",
    "cl2 = RandomForestClassifier(**rf_params)\n",
    "cl3 = GaussianNB()\n",
    "cl4 = DecisionTreeClassifier(max_depth=5)\n",
    "cl5 = CatBoostClassifier(task_type=\"GPU\", verbose=None, logging_level=\"Silent\")\n",
    "cl6 = LGBMClassifier(device=\"gpu\")\n",
    "\n",
    "# I used some hyperparameter search (ExtraTrees - Genetic search)\n",
    "cl7 = ExtraTreesClassifier(\n",
    "    bootstrap=False,\n",
    "    criterion=\"entropy\",\n",
    "    max_features=0.55,\n",
    "    min_samples_leaf=8,\n",
    "    min_samples_split=4,\n",
    "    n_estimators=100,\n",
    ")  # Optimized using TPOT\n",
    "cl8 = MLPClassifier(\n",
    "    activation=\"relu\",\n",
    "    alpha=0.1,\n",
    "    hidden_layer_sizes=(10, 10, 10),\n",
    "    learning_rate=\"constant\",\n",
    "    max_iter=2000,\n",
    "    random_state=RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adf0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForest\": cl2,\n",
    "    \"DecisionTree\": cl4,\n",
    "    \"CatBoost\": cl5,\n",
    "    \"LGBM\": cl6,\n",
    "    \"ExtraTrees\": cl7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a14415",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22267e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_results, models_names = list(), list() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf55d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>>> Training started <<<<\")\n",
    "for key in classifiers:\n",
    "    classifier = classifiers[key]\n",
    "    scores = model_selection.cross_val_score(\n",
    "        classifier, X_train, y_train, cv=FOLDS, scoring=\"accuracy\"\n",
    "    )\n",
    "    models_scores_results.append(scores)\n",
    "    models_names.append(key)\n",
    "    print(\"[%s] - accuracy: %0.5f \" % (key, scores.mean()))\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Save classifier for prediction\n",
    "    classifiers[key] = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "print('The nltk version is {}.'.format(nltk.__version__))\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f0c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "et = ExtraTreesClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "svc = SVC()\n",
    "rg = RidgeClassifier()\n",
    "clf_array = [rf, et, knn, svc, rg]\n",
    "for clf in clf_array:\n",
    "    vanilla_scores = cross_val_score(clf, X, y, cv=10, n_jobs=-1)\n",
    "    bagging_clf = BaggingClassifier(\n",
    "        clf, max_samples=0.4, max_features=10, random_state=42\n",
    "    )\n",
    "    bagging_scores = cross_val_score(bagging_clf, X, y, cv=10, n_jobs=-1)\n",
    "\n",
    "    print(\n",
    "        \"Mean of: {1:.3f}, std: (+/-) {2:.3f} [{0}]\".format(\n",
    "            clf.__class__.__name__, vanilla_scores.mean(), vanilla_scores.std()\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Mean of: {1:.3f}, std: (+/-) {2:.3f} [Bagging {0}]\\n\".format(\n",
    "            clf.__class__.__name__, bagging_scores.mean(), bagging_scores.std()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0578cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf = [rf, et, knn, svc, rg]\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"Random Forests\", rf),\n",
    "        (\"Extra Trees\", et),\n",
    "        (\"KNeighbors\", knn),\n",
    "        (\"SVC\", svc),\n",
    "        (\"Ridge Classifier\", rg),\n",
    "    ],\n",
    "    voting=\"hard\",\n",
    ")\n",
    "for clf, label in zip(\n",
    "    [rf, et, knn, svc, rg, eclf],\n",
    "    [\n",
    "        \"Random Forest\",\n",
    "        \"Extra Trees\",\n",
    "        \"KNeighbors\",\n",
    "        \"SVC\",\n",
    "        \"Ridge Classifier\",\n",
    "        \"Ensemble\",\n",
    "    ],\n",
    "):\n",
    "    scores = cross_val_score(clf, X, y, cv=10, scoring=\"accuracy\")\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d48edd",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb01a3",
   "metadata": {},
   "source": [
    "### Max Voting / Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble of Models\n",
    "estimator = []\n",
    "estimator.append(\n",
    "    (\"LR\", LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=200))\n",
    ")\n",
    "estimator.append((\"SVC\", SVC(gamma=\"auto\", probability=True)))\n",
    "estimator.append((\"DTC\", DecisionTreeClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7966a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier with hard voting\n",
    "hard_voting = VotingClassifier(estimators=estimator, voting=\"hard\")\n",
    "hard_voting.fit(X_train, y_train)\n",
    "y_pred = hard_voting.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a08987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
